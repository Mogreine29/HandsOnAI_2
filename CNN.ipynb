{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mogreine29/HandsOnAI_2/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "eRLBMkZJ28Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import gensim\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing import text, sequence\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFBjX8z-9uH3",
        "outputId": "8f220881-a0d0-4152-a776-527080531909"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive to access data and save models"
      ],
      "metadata": {
        "id": "Ga7SNUMI2_cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My\\ Drive/Challenge2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZJfyIHd94XB",
        "outputId": "8d6d1cf3-9fbe-421f-f774-8b33c20c1de8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Challenge2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading data "
      ],
      "metadata": {
        "id": "4ZKZXbyO3FHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Challenge2/fake_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Challenge2/fake_test.csv')"
      ],
      "metadata": {
        "id": "sKZc9ZUU-Xwc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing useless columns"
      ],
      "metadata": {
        "id": "SaruKXCD3fT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(['Unnamed: 0', 'target_name'], axis = 1)\n",
        "df_test = df_test.drop(['Unnamed: 0', 'target_name'], axis = 1)"
      ],
      "metadata": {
        "id": "14BIXzRZ-u-E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deleting parasite data (see report for more info)"
      ],
      "metadata": {
        "id": "Vmd_S97T0Dy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop([1136, 1180, 1317, 1362, 1429], inplace = True )"
      ],
      "metadata": {
        "id": "M0jY6EGZ0Dcr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding new data"
      ],
      "metadata": {
        "id": "BqzXxhbjJGGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_real = pd.read_csv(\"real_news.csv\")\n",
        "df_fake = pd.read_csv(\"fake_news.csv\")\n",
        "df_real = df_real.drop(['site', 'url', 'title'], axis = 1)\n",
        "df_fake = df_fake.drop(['site', 'url', 'title'], axis = 1)\n",
        "df_real['label'] = 0\n",
        "df_fake['label'] = 1\n",
        "df_real.rename(columns = {'text':'data'}, inplace = True)\n",
        "df_fake.rename(columns = {'text':'data'}, inplace = True)\n",
        "df_train = df_train.append(df_real, ignore_index = True)\n",
        "df_train = df_train.append(df_fake, ignore_index = True)\n",
        "\n",
        "df_train['data'] = df_train['data'].astype(str)\n",
        "df_test['data'] = df_test['data'].astype(str)"
      ],
      "metadata": {
        "id": "q5rqGS-nJFlz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stopwords and other characters using RegEx"
      ],
      "metadata": {
        "id": "luf5tA8e3h-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words('french'))\n",
        "moreStopwords = ['plus', 'comme', 'a', 'il', 'alors','au',\n",
        "'aucuns','aussi','autre','avant','avec','avoir','bon','car','ce','cela','ces','ceux','chaque','ci',\n",
        "'comme','comment','dans','des','du','dedans','dehors','depuis','devrait','doit','donc','dos','début',\n",
        "'elle','elles','en','encore','essai','est','et','eu','fait','faites','fois','font','hors','ici','il',\n",
        "'ils','je',\t'juste','la','le','les','leur','là','ma','maintenant','mais','mes','mien','moins','mon',\n",
        "'mot','même','ni','nommés','notre','nous','ou','où','par','parce','pas','peut','peu','plupart','pour',\n",
        "'pourquoi','quand','que','quel','quelle','quelles','quels','qui','sa','sans','ses','seulement','si',\n",
        "'sien','son','sont','sous','soyez',\t'sujet','sur','ta','tandis','tellement','tels','tes','ton','tous',\n",
        "'tout','trop','très','tu','voient','vont','votre','vous','vu','ça','étaient','état','étions','été','être',\n",
        "'cette','celle','dont','celui', 'adsbygoogle','window','secretnews','leurs','ainsi','toute','déjà','autres',\n",
        "'suivre','facebook','twitter','parodique', 'média','collaboratif', 'libre','notamment',\n",
        "'mercilesentreprisesgiletsjaunes',\n",
        "'entreprisesgiletsjaunes',\n",
        "'dontforgetyourmariage']\n",
        "\n",
        "STOPWORDS.update(moreStopwords)\n",
        "def cleaner(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub(\"\\\\W\",\" \",text) \n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)   \n",
        "    return text"
      ],
      "metadata": {
        "id": "x05OYt5q-2uF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['data'] = df_train['data'].apply(cleaner)\n",
        "df_test['data'] = df_test['data'].apply(cleaner)"
      ],
      "metadata": {
        "id": "-A22HZuf--6q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.append(df_test, ignore_index=True)\n",
        "shuffled = df_train.sample(frac=1).reset_index()"
      ],
      "metadata": {
        "id": "YRsC1CEvUnOl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, x_test, y, y_test = train_test_split(shuffled.data,shuffled.label,test_size=0.2,train_size=0.8)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x,y,test_size = 0.25,train_size =0.75)"
      ],
      "metadata": {
        "id": "oGjIDxPl_A3u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text vectorization using Keras"
      ],
      "metadata": {
        "id": "_YFfSsPW7CtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "maxlen = 512"
      ],
      "metadata": {
        "id": "mQ_-640dpbI9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "HOX7DAK-pdmR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_val = tokenizer.texts_to_sequences(x_val)\n",
        "X_val = sequence.pad_sequences(tokenized_val, maxlen=maxlen)\n",
        "\n",
        "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
        "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "G5wZ_n_EkskT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Matrix for our model"
      ],
      "metadata": {
        "id": "J3J4ziy07H6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin"
      ],
      "metadata": {
        "id": "cjEiawy1iYYH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model_fr = gensim.models.KeyedVectors.load_word2vec_format(\"frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\",binary=True)"
      ],
      "metadata": {
        "id": "bvcGKQctiaoF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = dict((key,word2vec_model_fr[key]) for key in word2vec_model_fr.vocab.keys())"
      ],
      "metadata": {
        "id": "HI8x3LR4qg7O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIv44SkZ8X3X",
        "outputId": "49233a9c-22a7-47c0-d088-e6a34c65ed46"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3249: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters of the Deep learning model"
      ],
      "metadata": {
        "id": "ua6toHuF7UnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 20\n",
        "embed_size = 200\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)"
      ],
      "metadata": {
        "id": "hSoIJ17hw0VS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model architecture"
      ],
      "metadata": {
        "id": "fuCtoKwM7YPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Neural Network\n",
        "model = Sequential()\n",
        "\n",
        "#Non-trainable embeddidng layer \n",
        "model.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "\n",
        "#LSTM \n",
        "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
        "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
        "model.add(Dense(units = 32 , activation = 'relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=Adam(learning_rate = 0.01), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd8VD-02xU4D",
        "outputId": "cdd707d7-3bea-4974-cdfe-a3668b9bf028"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.build()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaH9T8eS6Mvk",
        "outputId": "cc80755a-e7f7-46f6-af37-8f70bd022523"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 512, 200)          2000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 512, 128)          168448    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,219,969\n",
            "Trainable params: 219,969\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.fit()"
      ],
      "metadata": {
        "id": "vmPa1c6Y7gL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_val,y_val) , epochs = epochs , callbacks = [learning_rate_reduction])"
      ],
      "metadata": {
        "id": "0MZ4-bS16PhE",
        "outputId": "c8d74489-018b-43e0-b2b8-d6f1799b3963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "50/50 [==============================] - 259s 5s/step - loss: 0.3717 - accuracy: 0.8271 - val_loss: 0.2137 - val_accuracy: 0.9184 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 233s 5s/step - loss: 0.2117 - accuracy: 0.9174 - val_loss: 0.2013 - val_accuracy: 0.9182 - lr: 0.0100\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 235s 5s/step - loss: 0.1767 - accuracy: 0.9307 - val_loss: 0.2004 - val_accuracy: 0.9333 - lr: 0.0100\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 231s 5s/step - loss: 0.1436 - accuracy: 0.9454 - val_loss: 0.1466 - val_accuracy: 0.9469 - lr: 0.0100\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 233s 5s/step - loss: 0.1232 - accuracy: 0.9538 - val_loss: 0.1334 - val_accuracy: 0.9526 - lr: 0.0100\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 233s 5s/step - loss: 0.0824 - accuracy: 0.9681 - val_loss: 0.1291 - val_accuracy: 0.9561 - lr: 0.0100\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 228s 5s/step - loss: 0.0642 - accuracy: 0.9764 - val_loss: 0.1487 - val_accuracy: 0.9478 - lr: 0.0100\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9771\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "50/50 [==============================] - 230s 5s/step - loss: 0.0622 - accuracy: 0.9771 - val_loss: 0.1568 - val_accuracy: 0.9556 - lr: 0.0100\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 228s 5s/step - loss: 0.0358 - accuracy: 0.9880 - val_loss: 0.1760 - val_accuracy: 0.9530 - lr: 0.0050\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9892\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "50/50 [==============================] - 232s 5s/step - loss: 0.0290 - accuracy: 0.9892 - val_loss: 0.1778 - val_accuracy: 0.9547 - lr: 0.0050\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 239s 5s/step - loss: 0.0266 - accuracy: 0.9900 - val_loss: 0.1770 - val_accuracy: 0.9573 - lr: 0.0025\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 233s 5s/step - loss: 0.0186 - accuracy: 0.9934 - val_loss: 0.1870 - val_accuracy: 0.9587 - lr: 0.0025\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 226s 5s/step - loss: 0.0167 - accuracy: 0.9941 - val_loss: 0.1926 - val_accuracy: 0.9604 - lr: 0.0025\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 228s 5s/step - loss: 0.0155 - accuracy: 0.9945 - val_loss: 0.1941 - val_accuracy: 0.9602 - lr: 0.0025\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9957\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "50/50 [==============================] - 230s 5s/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.1937 - val_accuracy: 0.9594 - lr: 0.0025\n",
            "Epoch 16/20\n",
            "33/50 [==================>...........] - ETA: 1:17 - loss: 0.0098 - accuracy: 0.9964"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy test after training"
      ],
      "metadata": {
        "id": "JXvcqkRZ7oCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100 , \"%\")\n",
        "print(\"Accuracy of the model on Validating Data is - \" , model.evaluate(X_val,y_val)[1]*100 , \"%\")\n",
        "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
      ],
      "metadata": {
        "id": "6FRuhy0w6ROX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving model to google drive for further use"
      ],
      "metadata": {
        "id": "nbU7a4wq7r2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_save = '/content/drive/MyDrive/Challenge2/model20.h5'"
      ],
      "metadata": {
        "id": "xYWbZ0ewZzZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(path_to_save)"
      ],
      "metadata": {
        "id": "w25hf1TS8K9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "wrvqR8cY5U6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphic of model training\n"
      ],
      "metadata": {
        "id": "E9OnCLHM7uSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = [i for i in range(20)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "fig.set_size_inches(20,10)\n",
        "\n",
        "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
        "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\n",
        "ax[0].set_title('Training & Testing Accuracy')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "ax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\n",
        "ax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\n",
        "ax[1].set_title('Training & Testing Loss')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uNJQML7v6TLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction on some data"
      ],
      "metadata": {
        "id": "lTjwSYus8JpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "Cc0ct1gl6Vv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix"
      ],
      "metadata": {
        "id": "YfLO2UqN8MTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(list(y_test), np.round(pred).astype(int), target_names = ['Not Fake','Fake']))"
      ],
      "metadata": {
        "id": "5K9SVmvk6X6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test,np.round(pred).astype(int))\n",
        "cm"
      ],
      "metadata": {
        "id": "IxaCAKih6Y4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = pd.DataFrame(cm , index = ['Fake','Original'] , columns = ['Fake','Original'])"
      ],
      "metadata": {
        "id": "zbIOO-CX6Z9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(cm,cmap= \"Reds\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Fake','Original'] , yticklabels = ['Fake','Original'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")"
      ],
      "metadata": {
        "id": "r9NtosEZ6bDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "model22 = keras.models.load_model(path_to_save)"
      ],
      "metadata": {
        "id": "MUcpA8ryYf8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of the model on Testing Data is - \" , model22.evaluate(X_test,y_test)[1]*100 , \"%\")"
      ],
      "metadata": {
        "id": "iwKrSByDYoDW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}